{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "Fastai with ðŸ¤— Transformers (BERT, RoBERTa, ...).ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tt32_uou54Kk",
        "colab_type": "text"
      },
      "source": [
        "## JanataHack: NLP Hackathon\n",
        "My first NLP challenge\n",
        "\n",
        "Trust me it was a great learning experience and but atlast I turned to be unlucky person on leaderboard.\n",
        "\n",
        "Anyways, \"You live and you learn xD\", said by Baba Ranchoddas(KK_sir)\n",
        "\n",
        "Hope you will enjoy this notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Uwe9wuOx54Kl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "pip install -q transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "02mRzBSu54Kp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from pathlib import Path \n",
        "\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "import random \n",
        "\n",
        "# fastai\n",
        "from fastai import *\n",
        "from fastai.text import *\n",
        "from fastai.callbacks import *\n",
        "\n",
        "# transformers\n",
        "from transformers import PreTrainedModel, PreTrainedTokenizer, PretrainedConfig\n",
        "\n",
        "from transformers import BertForSequenceClassification, BertTokenizer, BertConfig\n",
        "from transformers import RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig\n",
        "from transformers import XLNetForSequenceClassification, XLNetTokenizer, XLNetConfig\n",
        "from transformers import XLMForSequenceClassification, XLMTokenizer, XLMConfig\n",
        "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "FcgoAUzg54Kx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_ROOT = Path(\"..\") / \"/content/janta-nlp/\"\n",
        "train = pd.read_csv(DATA_ROOT / 'train.csv')\n",
        "test = pd.read_csv(DATA_ROOT / 'test.csv')\n",
        "print(train.shape,test.shape)\n",
        "train.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0t3DwtI-wpW",
        "colab_type": "text"
      },
      "source": [
        "#### I had approached this problem as simple text classification problem and I had used only User_review for this,"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugWy9DhU_E17",
        "colab_type": "text"
      },
      "source": [
        "I had used Roberta Model using HuggingFace library.\n",
        "Actually, this is generic implementation of various state of the art NLP models like BERT,RobertA,XLNet. So you can easily switch between any of them\n",
        "\n",
        "### So Let Begin..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Y73VGig54K0",
        "colab_type": "text"
      },
      "source": [
        "## Main transformers classes\n",
        "In ``transformers``, each model architecture is associated with 3 main types of classes:\n",
        "* A **model class** to load/store a particular pre-train model.\n",
        "* A **tokenizer class** to pre-process the data and make it compatible with a particular model.\n",
        "* A **configuration class** to load/store the configuration of a particular model.\n",
        "\n",
        "For example, if you want to use the Bert architecture for text classification, you would use [``BertForSequenceClassification``](https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification) for the **model class**, [``BertTokenizer``](https://huggingface.co/transformers/model_doc/bert.html#berttokenizer) for the **tokenizer class** and [``BertConfig``](https://huggingface.co/transformers/model_doc/bert.html#bertconfig) for the **configuration class**.Â \n",
        "\n",
        "In order to switch easily between classes â€Š-â€Š each related to a specific model type â€Š-â€Š I created a dictionary that allows loading the correct classes by just specifying the correct model type name."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "CsUkPcv354K1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "e60c797c-a015-4147-95f1-3cac8020c46d"
      },
      "source": [
        "MODEL_CLASSES = {\n",
        "    'bert': (BertForSequenceClassification, BertTokenizer, BertConfig),\n",
        "    'xlnet': (XLNetForSequenceClassification, XLNetTokenizer, XLNetConfig),\n",
        "    'xlm': (XLMForSequenceClassification, XLMTokenizer, XLMConfig),\n",
        "    'roberta': (RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig),\n",
        "    'distilbert': (DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig)\n",
        "}"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a3372f1e1b04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m MODEL_CLASSES = {\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;34m'bert'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBertForSequenceClassification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;34m'xlnet'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mXLNetForSequenceClassification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXLNetTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXLNetConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m'xlm'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mXLMForSequenceClassification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXLMTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXLMConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m'roberta'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mRobertaForSequenceClassification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRobertaTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRobertaConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'BertForSequenceClassification' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylAtz5Ne54K3",
        "colab_type": "text"
      },
      "source": [
        "You will see later, that those classes share a common class method ``from_pretrained(pretrained_model_name,Â ...)``. In our case, the parameter ``pretrained_model_name`` is a string with the shortcut name of a pre-trained model/tokenizer/configuration to load, e.g ``'bert-base-uncased'``. We can find all the shortcut names in the transformers documentation [here](https://huggingface.co/transformers/pretrained_models.html#pretrained-models)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "N3P1QLPx54K4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Parameters\n",
        "seed = 42\n",
        "use_fp16 = False\n",
        "bs = 16 \n",
        "\n",
        "model_type = 'roberta'\n",
        "pretrained_model_name = 'roberta-base'\n",
        "\n",
        "# model_type = 'bert'\n",
        "# pretrained_model_name='bert-base-uncased'\n",
        "\n",
        "#model_type = 'distilbert'\n",
        "#pretrained_model_name = 'distilbert-base-uncased'\n",
        "\n",
        "#model_type = 'xlm'\n",
        "#pretrained_model_name = 'xlm-clm-enfr-1024'\n",
        "\n",
        "#model_type = 'xlnet'\n",
        "#pretrained_model_name = 'xlnet-base-cased'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "pITwot1i54K6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_class, tokenizer_class, config_class = MODEL_CLASSES[model_type]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Wdg2ags54K8",
        "colab_type": "text"
      },
      "source": [
        "Print the available values for ``pretrained_model_name`` (shortcut names) corresponding to the ``model_type`` used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ytlxB-SD54K8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_class.pretrained_model_archive_map.keys()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80_T_5LC54LA",
        "colab_type": "text"
      },
      "source": [
        "It is worth noting that in this case, we use the ``transformers`` library only for a multi-class text classification task. For that reason, this tutorial integrates only the transformer architectures that have a model for sequence classification implemented. These model types areÂ :\n",
        "* BERT (from Google)\n",
        "* XLNet (from Google/CMU)\n",
        "* XLM (from Facebook)\n",
        "* RoBERTa (from Facebook)\n",
        "* DistilBERT (from HuggingFace)\n",
        "\n",
        "However, if you want to go furtherâ€Š-â€Šby implementing another type of model or NLP taskâ€Š-â€Šthis tutorial still an excellent starter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JseoXP854LB",
        "colab_type": "text"
      },
      "source": [
        "## Util function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqoGimmw54LB",
        "colab_type": "text"
      },
      "source": [
        "Function to set the seed for generating random numbers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "bXE7y2a354LC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def seed_all(seed_value):\n",
        "    random.seed(seed_value) # Python\n",
        "    np.random.seed(seed_value) # cpu vars\n",
        "    torch.manual_seed(seed_value) # cpu  vars\n",
        "    \n",
        "    if torch.cuda.is_available(): \n",
        "        torch.cuda.manual_seed(seed_value)\n",
        "        torch.cuda.manual_seed_all(seed_value) # gpu vars\n",
        "        torch.backends.cudnn.deterministic = True  #needed\n",
        "        torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "c_QdMZmY54LE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed_all(seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QITJj3I254LG",
        "colab_type": "text"
      },
      "source": [
        "## Data pre-processing\n",
        "\n",
        "To match pre-training, we have to format the model input sequence in a specific format.\n",
        "To do so, you have to first **tokenize** and then **numericalize** the texts correctly.\n",
        "The difficulty here is that each pre-trained model, that we will fine-tune, requires exactly the same specific pre-processâ€Š-â€Š**tokenization** & **numericalization**â€Š-â€Šthan the pre-process used during the pre-train part.\n",
        "Fortunately, the **tokenizer class** from ``transformers`` provides the correct pre-process tools that correspond to each pre-trained model.\n",
        "\n",
        "In the ``fastai`` library, data pre-processing is done automatically during the creation of the ``DataBunch``. \n",
        "As you will see in the ``DataBunch`` implementation, the **tokenizer** and **numericalizer** are passed in the processor argument under the following format :\n",
        "\n",
        "``processor = [TokenizeProcessor(tokenizer=tokenizer,...), NumericalizeProcessor(vocab=vocab,...)]``\n",
        "\n",
        "Let's first analyse how we can integrate the ``transformers`` **tokenizer** within the ``TokenizeProcessor`` function.\n",
        "\n",
        "### Custom Tokenizer\n",
        "This part can be a little bit confusing because a lot of classes are wrapped in each other and with similar names.\n",
        "To resume, if we look attentively at the ``fastai`` implementation, we notice thatÂ :\n",
        "1. The [``TokenizeProcessor`` object](https://docs.fast.ai/text.data.html#TokenizeProcessor) takes as ``tokenizer`` argument a ``Tokenizer`` object.\n",
        "2. The [``Tokenizer`` object](https://docs.fast.ai/text.transform.html#Tokenizer) takes as ``tok_func`` argument a ``BaseTokenizer`` object.\n",
        "3. The [``BaseTokenizer`` object](https://docs.fast.ai/text.transform.html#BaseTokenizer) implement the function ``tokenizer(t:str) â†’ List[str]`` that take a text ``t`` and returns the list of its tokens.\n",
        "\n",
        "Therefore, we can simply create a new class ``TransformersBaseTokenizer`` that inherits from ``BaseTokenizer`` and overwrite a new ``tokenizer`` function.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "LY_vAEoi54LH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TransformersBaseTokenizer(BaseTokenizer):\n",
        "    \"\"\"Wrapper around PreTrainedTokenizer to be compatible with fast.ai\"\"\"\n",
        "    def __init__(self, pretrained_tokenizer: PreTrainedTokenizer, model_type = 'bert', **kwargs):\n",
        "        self._pretrained_tokenizer = pretrained_tokenizer\n",
        "        self.max_seq_len = pretrained_tokenizer.max_len\n",
        "        self.model_type = model_type\n",
        "\n",
        "    def __call__(self, *args, **kwargs): \n",
        "        return self\n",
        "\n",
        "    def tokenizer(self, t:str) -> List[str]:\n",
        "        \"\"\"Limits the maximum sequence length and add the spesial tokens\"\"\"\n",
        "        CLS = self._pretrained_tokenizer.cls_token\n",
        "        SEP = self._pretrained_tokenizer.sep_token\n",
        "        if self.model_type in ['roberta']:\n",
        "            tokens = self._pretrained_tokenizer.tokenize(t, add_prefix_space=True)[:self.max_seq_len - 2]\n",
        "            tokens = [CLS] + tokens + [SEP]\n",
        "        else:\n",
        "            tokens = self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2]\n",
        "            if self.model_type in ['xlnet']:\n",
        "                tokens = tokens + [SEP] +  [CLS]\n",
        "            else:\n",
        "                tokens = [CLS] + tokens + [SEP]\n",
        "        return tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "2xNVCvk154LJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transformer_tokenizer = tokenizer_class.from_pretrained(pretrained_model_name)\n",
        "transformer_base_tokenizer = TransformersBaseTokenizer(pretrained_tokenizer = transformer_tokenizer, model_type = model_type)\n",
        "fastai_tokenizer = Tokenizer(tok_func = transformer_base_tokenizer, pre_rules=[], post_rules=[])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVyYE1v454LL",
        "colab_type": "text"
      },
      "source": [
        "### Custom Numericalizer\n",
        "\n",
        "In ``fastai``, [``NumericalizeProcessor``  object](https://docs.fast.ai/text.data.html#NumericalizeProcessor) takes as ``vocab`` argument a [``Vocab`` object](https://docs.fast.ai/text.transform.html#Vocab). \n",
        "From this analyse, we suggest two ways to adapt the fastai numericalizer:\n",
        "1. You can, like decribed in the [Dev Sharma's article](https://medium.com/analytics-vidhya/using-roberta-with-fastai-for-nlp-7ed3fed21f6c) (Section *1. Setting Up the Tokenizer*), retreive the list of tokens and create a ``Vocab`` object.\n",
        "2. Create a new class ``TransformersVocab`` that inherits from ``Vocab`` and overwrite ``numericalize`` and ``textify`` functions.\n",
        "\n",
        "Even if the first solution seems to be simpler, ``Transformers`` does not provide, for all models, a straightforward way to retreive his list of tokens. \n",
        "Therefore, I implemented the second solution, which runs for each model type.\n",
        "It consists of using the functions ``convert_tokens_to_ids`` and ``convert_ids_to_tokens`` in respectively ``numericalize`` and ``textify``."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "B5iJ2FX554LL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TransformersVocab(Vocab):\n",
        "    def __init__(self, tokenizer: PreTrainedTokenizer):\n",
        "        super(TransformersVocab, self).__init__(itos = [])\n",
        "        self.tokenizer = tokenizer\n",
        "    \n",
        "    def numericalize(self, t:Collection[str]) -> List[int]:\n",
        "        \"Convert a list of tokens `t` to their ids.\"\n",
        "        return self.tokenizer.convert_tokens_to_ids(t)\n",
        "        #return self.tokenizer.encode(t)\n",
        "\n",
        "    def textify(self, nums:Collection[int], sep=' ') -> List[str]:\n",
        "        \"Convert a list of `nums` to their tokens.\"\n",
        "        nums = np.array(nums).tolist()\n",
        "        return sep.join(self.tokenizer.convert_ids_to_tokens(nums)) if sep is not None else self.tokenizer.convert_ids_to_tokens(nums)\n",
        "    \n",
        "    def __getstate__(self):\n",
        "        return {'itos':self.itos, 'tokenizer':self.tokenizer}\n",
        "\n",
        "    def __setstate__(self, state:dict):\n",
        "        self.itos = state['itos']\n",
        "        self.tokenizer = state['tokenizer']\n",
        "        self.stoi = collections.defaultdict(int,{v:k for k,v in enumerate(self.itos)})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrSeS0-b54LN",
        "colab_type": "text"
      },
      "source": [
        "NB: The functions ``__gestate__`` and ``__setstate__`` allow the functions [export](https://docs.fast.ai/basic_train.html#Learner.export) and [load_learner](https://docs.fast.ai/basic_train.html#load_learner) to work correctly with ``TransformersVocab``."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fb87n2sV54LO",
        "colab_type": "text"
      },
      "source": [
        "### Custom processor\n",
        "Now that we have our custom **tokenizer** and **numericalizer**, we can create the custom **processor**. Notice we are passing the ``include_bos = False`` and ``include_eos = False`` options. This is because ``fastai`` adds its own special tokens by default which interferes with the ``[CLS]`` and ``[SEP]`` tokens added by our custom tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "nvorPxW854LO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transformer_vocab =  TransformersVocab(tokenizer = transformer_tokenizer)\n",
        "numericalize_processor = NumericalizeProcessor(vocab=transformer_vocab)\n",
        "\n",
        "tokenize_processor = TokenizeProcessor(tokenizer=fastai_tokenizer, include_bos=False, include_eos=False)\n",
        "\n",
        "transformer_processor = [tokenize_processor, numericalize_processor]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r73t_dli54LR",
        "colab_type": "text"
      },
      "source": [
        "## Setting up the Databunch\n",
        "For the DataBunch creation, you have to pay attention to set the processor argument to our new custom processor ``transformer_processor`` and manage correctly the padding.\n",
        "\n",
        "As mentioned in the HuggingFace documentation, BERT, RoBERTa, XLM and DistilBERT are models with absolute position embeddings, so it's usually advised to pad the inputs on the right rather than the left. Regarding XLNET, it is a model with relative position embeddings, therefore, you can either pad the inputs on the right or on the left."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "uARpLggy54LR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pad_first = bool(model_type in ['xlnet'])\n",
        "pad_idx = transformer_tokenizer.pad_token_id"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "3NlergJV54LU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokens = transformer_tokenizer.tokenize('Salut c est moi, Hello it s me')\n",
        "print(tokens)\n",
        "ids = transformer_tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(ids)\n",
        "transformer_tokenizer.convert_ids_to_tokens(ids)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLZ7fIW654LW",
        "colab_type": "text"
      },
      "source": [
        "There is multible ways to create a DataBunch, in our implementation, we use [the data block API](https://docs.fast.ai/data_block.html#The-data-block-API), which gives more flexibility."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "k-dIcH5o54LW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#user_review\tuser_suggestion\n",
        "databunch = (TextList.from_df(train, cols='user_review', processor=transformer_processor)\n",
        "             .split_by_rand_pct(0.1,seed=seed)\n",
        "             .label_from_df(cols= 'user_suggestion')\n",
        "             .add_test(test)\n",
        "             .databunch(bs=bs, pad_first=pad_first, pad_idx=pad_idx))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDcGA9x054LY",
        "colab_type": "text"
      },
      "source": [
        "Check batch and tokenizer :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "mnQKfz-n54LY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('[CLS] token :', transformer_tokenizer.cls_token)\n",
        "print('[SEP] token :', transformer_tokenizer.sep_token)\n",
        "print('[PAD] token :', transformer_tokenizer.pad_token)\n",
        "databunch.show_batch()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08v4fsdF54La",
        "colab_type": "text"
      },
      "source": [
        "Check batch and numericalizer :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "yFZo4abe54La",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('[CLS] id :', transformer_tokenizer.cls_token_id)\n",
        "print('[SEP] id :', transformer_tokenizer.sep_token_id)\n",
        "print('[PAD] id :', pad_idx)\n",
        "test_one_batch = databunch.one_batch()[0]\n",
        "print('Batch shape : ',test_one_batch.shape)\n",
        "print(test_one_batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q82zLzEH54Lc",
        "colab_type": "text"
      },
      "source": [
        "### Custom model\n",
        "As mentioned [here](https://github.com/huggingface/transformers#models-always-output-tuples), every model's forward method always outputs a ``tuple`` with various elements depending on the model and the configuration parameters. In our case, we are interested to access only to the logits.Â \n",
        "One way to access them is to create a custom model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "bWgGvuTg54Ld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# defining our model architecture \n",
        "class CustomTransformerModel(nn.Module):\n",
        "    def __init__(self, transformer_model: PreTrainedModel):\n",
        "        super(CustomTransformerModel,self).__init__()\n",
        "        self.transformer = transformer_model\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        \n",
        "        # attention_mask\n",
        "        # Mask to avoid performing attention on padding token indices.\n",
        "        # Mask values selected in ``[0, 1]``:\n",
        "        # ``1`` for tokens that are NOT MASKED, ``0`` for MASKED tokens.\n",
        "        attention_mask = (input_ids!=pad_idx).type(input_ids.type()) \n",
        "        \n",
        "        logits = self.transformer(input_ids,\n",
        "                                  attention_mask = attention_mask)[0]   \n",
        "        return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ct9X4Mt54Lf",
        "colab_type": "text"
      },
      "source": [
        "To make our transformers adapted to multiclass classification, before loading the pre-trained model, we need to precise the number of labels. To do so, you can modify the config instance or either modify like in [Keita Kurita's article](https://mlexplained.com/2019/05/13/a-tutorial-to-fine-tuning-bert-with-fast-ai/) (Section: *Initializing the Learner*) the ``num_labels`` argument."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ie5qMDWB54Lf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = config_class.from_pretrained(pretrained_model_name)\n",
        "config.num_labels = 2\n",
        "config.use_bfloat16 = use_fp16\n",
        "print(config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "_aMsF0C054Lh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transformer_model = model_class.from_pretrained(pretrained_model_name, config = config)\n",
        "# transformer_model = model_class.from_pretrained(pretrained_model_name, num_labels = 5)\n",
        "\n",
        "custom_transformer_model = CustomTransformerModel(transformer_model = transformer_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABInjGdV54Lm",
        "colab_type": "text"
      },
      "source": [
        "## LearnerÂ : Custom Optimizer / CustomÂ Metric\n",
        "In ``pytorch-transformers``, HuggingFace had implemented two specific optimizers â€Š-â€Š BertAdam and OpenAIAdam â€Š-â€Š that have been replaced by a single AdamW optimizer.\n",
        "This optimizer matches Pytorch Adam optimizer Api, therefore, it becomes straightforward to integrate it within ``fastai``.\n",
        "It is worth noting that for reproducing BertAdam specific behavior, you have to set ``correct_bias = False``.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "yBx5Qnq754Lm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from fastai.callbacks import *\n",
        "from transformers import AdamW\n",
        "from functools import partial\n",
        "\n",
        "CustomAdamW = partial(AdamW, correct_bias=False)\n",
        "\n",
        "learner = Learner(databunch, \n",
        "                  custom_transformer_model, \n",
        "                  opt_func = CustomAdamW, \n",
        "                  metrics=[accuracy, error_rate])\n",
        "\n",
        "# Show graph of learner stats and metrics after each epoch.\n",
        "learner.callbacks.append(ShowGraph(learner))\n",
        "\n",
        "# Put learn in FP16 precision mode. --> Seems to not working\n",
        "if use_fp16: learner = learner.to_fp16()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaB9I5wr54Lq",
        "colab_type": "text"
      },
      "source": [
        "We can decide to divide the model in 14 blocksÂ :\n",
        "* 1 Embedding\n",
        "* 12 transformer\n",
        "* 1 classifier\n",
        "\n",
        "In this case, we can split our model in this wayÂ :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "dFYuQxDu54Lq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''# For DistilBERT\n",
        "list_layers = [learner.model.transformer.distilbert.embeddings,\n",
        "                learner.model.transformer.distilbert.transformer.layer[0],\n",
        "                learner.model.transformer.distilbert.transformer.layer[1],\n",
        "                learner.model.transformer.distilbert.transformer.layer[2],\n",
        "                learner.model.transformer.distilbert.transformer.layer[3],\n",
        "                learner.model.transformer.distilbert.transformer.layer[4],\n",
        "                learner.model.transformer.distilbert.transformer.layer[5],\n",
        "                learner.model.transformer.pre_classifier]\n",
        "'''\n",
        "#For xlnet-base-cased\n",
        "'''list_layers = [learner.model.transformer.transformer.word_embedding,\n",
        "               learner.model.transformer.transformer.layer[0],\n",
        "               learner.model.transformer.transformer.layer[1],\n",
        "               learner.model.transformer.transformer.layer[2],\n",
        "               learner.model.transformer.transformer.layer[3],\n",
        "               learner.model.transformer.transformer.layer[4],\n",
        "               learner.model.transformer.transformer.layer[5],\n",
        "               learner.model.transformer.transformer.layer[6],\n",
        "               learner.model.transformer.transformer.layer[7],\n",
        "               learner.model.transformer.transformer.layer[8],\n",
        "               learner.model.transformer.transformer.layer[9],\n",
        "               learner.model.transformer.transformer.layer[10],\n",
        "               learner.model.transformer.transformer.layer[11],\n",
        "               learner.model.transformer.sequence_summary]\n",
        "'''\n",
        "# For roberta-base\n",
        "list_layers = [learner.model.transformer.roberta.embeddings,\n",
        "              learner.model.transformer.roberta.encoder.layer[0],\n",
        "              learner.model.transformer.roberta.encoder.layer[1],\n",
        "              learner.model.transformer.roberta.encoder.layer[2],\n",
        "              learner.model.transformer.roberta.encoder.layer[3],\n",
        "              learner.model.transformer.roberta.encoder.layer[4],\n",
        "              learner.model.transformer.roberta.encoder.layer[5],\n",
        "              learner.model.transformer.roberta.encoder.layer[6],\n",
        "              learner.model.transformer.roberta.encoder.layer[7],\n",
        "              learner.model.transformer.roberta.encoder.layer[8],\n",
        "              learner.model.transformer.roberta.encoder.layer[9],\n",
        "              learner.model.transformer.roberta.encoder.layer[10],\n",
        "              learner.model.transformer.roberta.encoder.layer[11],\n",
        "              learner.model.transformer.roberta.pooler]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8vMJ1JJ54Ls",
        "colab_type": "text"
      },
      "source": [
        "Check groups : "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "4pvjsNIG54Ls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learner.split(list_layers)\n",
        "num_groups = len(learner.layer_groups)\n",
        "print('Learner split in',num_groups,'groups')\n",
        "print(learner.layer_groups)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFyyo1yB54Lu",
        "colab_type": "text"
      },
      "source": [
        "## Train\n",
        "Now we can finally use all the fastai build-in features to train our model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "NiyHubjm54ME",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = 1e-5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "H72jpsXA54MQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed_all(seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Bk0w1DLb54MU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learner.fit_one_cycle(2, max_lr=slice(lr*0.95**num_groups, lr), moms=(0.8, 0.9))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AT5qc8MS54MY",
        "colab_type": "text"
      },
      "source": [
        "Now, you can predict examples with:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "F9AlusoI54MY",
        "colab_type": "code",
        "outputId": "041738b8-5cb1-45b9-e34f-8369c005dff2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "learner.predict('This is the best game of 2020')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Category 1, tensor(1), tensor([0.0048, 0.9952]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "FOQAgtid54Mb",
        "colab_type": "code",
        "outputId": "a42cd3e5-96fc-490b-91c7-fc975595a3ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "learner.predict('This is the worst game of 2020')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Category 0, tensor(0), tensor([0.9818, 0.0182]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNc7DbLv54Me",
        "colab_type": "text"
      },
      "source": [
        "## Export Learner\n",
        "In order to export and load the learner you can do these operations:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "LqaFR52c54Me",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learner.export(file = 'transformer.pkl');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "4cDniHzR54Mf",
        "colab_type": "code",
        "outputId": "f5bef803-8591-46b1-e23b-01a1f762daa2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "source": [
        "path = '/content'\n",
        "export_learner = load_learner(path, file = 'transformer.pkl')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpS8WPlD54Mg",
        "colab_type": "text"
      },
      "source": [
        "As mentioned [here](https://docs.fast.ai/basic_train.html#load_learner), you have to be careful that each custom classes - like ``TransformersVocab`` - are first defined before executing ``load_learner``."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ClHh1BTQ54Mh",
        "colab_type": "code",
        "outputId": "e99520bb-d37c-49b1-9d22-8c00ae4b4f69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "export_learner.predict('This is the worst game of 2020')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Category 0, tensor(0), tensor([0.9213, 0.0787]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouaIH95N54Mi",
        "colab_type": "text"
      },
      "source": [
        "## Creating prediction\n",
        "Now that the model is trained, we want to generate predictions from the test dataset.\n",
        "\n",
        "As specified in Keita Kurita's [article](https://mlexplained.com/2019/05/13/a-tutorial-to-fine-tuning-bert-with-fast-ai/), as the function ``get_preds`` does not return elements in order by default, you will have to resort the elements into their correct order."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "mn6WThMP54Mi",
        "colab_type": "code",
        "outputId": "d60935ab-ff40-49b6-8764-29c21150ecab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "source": [
        "def get_preds_as_nparray(ds_type) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    the get_preds method does not yield the elements in order by default\n",
        "    we borrow the code from the RNNLearner to resort the elements into their correct order\n",
        "    \"\"\"\n",
        "    preds = learner.get_preds(ds_type)[0].detach().cpu().numpy()\n",
        "    sampler = [i for i in databunch.dl(ds_type).sampler]\n",
        "    reverse_sampler = np.argsort(sampler)\n",
        "    return preds[reverse_sampler, :]\n",
        "\n",
        "test_preds = get_preds_as_nparray(DatasetType.Test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "oVTeeXNu54Ml",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_submission = pd.read_csv(DATA_ROOT / 'sub.csv')\n",
        "sample_submission['user_suggestion'] = np.argmax(test_preds,axis=1)\n",
        "sample_submission.to_csv(\"try_robertA.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foAI-5VakIje",
        "colab_type": "code",
        "outputId": "e3c67cc8-8b04-4966-c521-fb5800e7f136",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "sample_submission['user_suggestion'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    4651\n",
              "0    3394\n",
              "Name: user_suggestion, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "KDL1xAQ454Mp",
        "colab_type": "code",
        "outputId": "2fbb4976-748b-4055-fea8-897a5ae51db4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "sample_submission.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review_id</th>\n",
              "      <th>user_suggestion</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1603</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1604</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1605</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1606</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1607</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   review_id  user_suggestion\n",
              "0       1603                1\n",
              "1       1604                0\n",
              "2       1605                0\n",
              "3       1606                0\n",
              "4       1607                1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "1aWa1fpU54Mq",
        "colab_type": "code",
        "outputId": "9918e278-f56f-4260-d134-49d7dd810cf1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "def create_download_link(title = \"Download CSV file\", filename = \"data.csv\"):  \n",
        "    html = '<a href={filename}>{title}</a>'\n",
        "    html = html.format(title=title,filename=filename)\n",
        "    return HTML(html)\n",
        "\n",
        "# create a link to download the dataframe which was saved with .to_csv method\n",
        "create_download_link(filename='robertA.csv')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<a href=robertA.csv>Download CSV file</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fml_rLwA_za",
        "colab_type": "text"
      },
      "source": [
        "### Ensemble Trick\n",
        "\n",
        "I had trained the same model with three different seed values and ensembled them and hopefully. It worked and I was able to acheive the below scores.\n",
        "\n",
        "Public score: 0.930437173494639\n",
        "\n",
        "Private score: 0.934822409373856\n",
        "\n",
        "But, unfortunately I had didn't choose this as my final solution. \n",
        "\n",
        "I choose unensemabled version as final solution and it was able to achieve 0.928467153284672 as Private Score which helped me to achieve 16th rank on the Private Leaderboard and 10th rank on Public Leaderboard.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9iXAs6Y54Mr",
        "colab_type": "text"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "In this NoteBook, I explain how to combine the ``transformers`` library with the beloved ``fastai`` library. \n",
        "\n",
        "This year, the transformers became an essential tool to NLP. \n",
        "I hope you enjoyed this and found it useful.Â \n",
        "Thanks for reading and don't hesitate in leaving questions or suggestions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0aB0oQy54Mr",
        "colab_type": "text"
      },
      "source": [
        "# References\n",
        "* Hugging Face, Transformers GitHub (Nov 2019), [https://github.com/huggingface/transformers](https://github.com/huggingface/transformers)\n",
        "* Fast.ai, Fastai documentation (Nov 2019), [https://docs.fast.ai/text.html](https://docs.fast.ai/text.html)\n",
        "* Jeremy Howard & Sebastian Ruder, Universal Language Model Fine-tuning for Text Classification (May 2018), [https://arxiv.org/abs/1801.06146](https://arxiv.org/abs/1801.06146)\n",
        "* Keita Kurita's articleÂ : [A Tutorial to Fine-Tuning BERT with Fast AI](https://mlexplained.com/2019/05/13/a-tutorial-to-fine-tuning-bert-with-fast-ai/)Â (May 2019)\n",
        "* Dev Sharma's articleÂ : [Using RoBERTa with Fastai for NLP](https://medium.com/analytics-vidhya/using-roberta-with-fastai-for-nlp-7ed3fed21f6c) (Sep 2019)"
      ]
    }
  ]
}